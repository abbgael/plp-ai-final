# Pneumonia Detection in Chest X-rays using Deep Learning
# Complete pipeline for training, evaluation, and visualization

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import cv2
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from sklearn.utils.class_weight import compute_class_weight
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16, ResNet50
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
import warnings
warnings.filterwarnings('ignore')

# Mount Google Drive (uncomment if running in Google Colab)
# from google.colab import drive
# drive.mount('/content/drive')

# Configuration
class Config:
    # Update this path to your Google Drive folder
    BASE_PATH = '/content/drive/MyDrive/chest_xray'  # Change this to your actual path
    
    # Image parameters
    IMG_SIZE = (224, 224)
    BATCH_SIZE = 32
    
    # Training parameters
    EPOCHS = 50
    LEARNING_RATE = 0.0001
    
    # Model parameters
    MODEL_NAME = 'pneumonia_detector'

config = Config()

def setup_data_paths():
    """Setup data paths for train, validation, and test sets"""
    train_dir = os.path.join(config.BASE_PATH, 'train')
    val_dir = os.path.join(config.BASE_PATH, 'val')
    test_dir = os.path.join(config.BASE_PATH, 'test')
    
    return train_dir, val_dir, test_dir

def analyze_dataset_demographics(train_dir, val_dir, test_dir):
    """Analyze and visualize dataset demographics"""
    print("üìä DATASET DEMOGRAPHICS ANALYSIS")
    print("=" * 50)
    
    # Count images in each category
    demographics = {}
    
    for split, path in [('Train', train_dir), ('Validation', val_dir), ('Test', test_dir)]:
        normal_path = os.path.join(path, 'NORMAL')
        pneumonia_path = os.path.join(path, 'PNEUMONIA')
        
        normal_count = len(os.listdir(normal_path)) if os.path.exists(normal_path) else 0
        pneumonia_count = len(os.listdir(pneumonia_path)) if os.path.exists(pneumonia_path) else 0
        
        demographics[split] = {
            'Normal': normal_count,
            'Pneumonia': pneumonia_count,
            'Total': normal_count + pneumonia_count
        }
        
        print(f"{split} Set:")
        print(f"  Normal: {normal_count}")
        print(f"  Pneumonia: {pneumonia_count}")
        print(f"  Total: {normal_count + pneumonia_count}")
        print(f"  Pneumonia Ratio: {pneumonia_count/(normal_count + pneumonia_count):.2%}\n")
    
    # Create visualization
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Bar plot of counts
    df_demo = pd.DataFrame(demographics).T
    df_demo[['Normal', 'Pneumonia']].plot(kind='bar', ax=axes[0,0], color=['skyblue', 'salmon'])
    axes[0,0].set_title('Dataset Distribution by Split')
    axes[0,0].set_ylabel('Number of Images')
    axes[0,0].legend()
    axes[0,0].tick_params(axis='x', rotation=45)
    
    # Pie chart for overall distribution
    total_normal = sum([demographics[split]['Normal'] for split in demographics])
    total_pneumonia = sum([demographics[split]['Pneumonia'] for split in demographics])
    
    axes[0,1].pie([total_normal, total_pneumonia], labels=['Normal', 'Pneumonia'], 
                  autopct='%1.1f%%', colors=['skyblue', 'salmon'])
    axes[0,1].set_title('Overall Dataset Distribution')
    
    # Stacked bar chart
    splits = list(demographics.keys())
    normal_counts = [demographics[split]['Normal'] for split in splits]
    pneumonia_counts = [demographics[split]['Pneumonia'] for split in splits]
    
    axes[1,0].bar(splits, normal_counts, label='Normal', color='skyblue')
    axes[1,0].bar(splits, pneumonia_counts, bottom=normal_counts, label='Pneumonia', color='salmon')
    axes[1,0].set_title('Stacked Distribution by Split')
    axes[1,0].set_ylabel('Number of Images')
    axes[1,0].legend()
    
    # Class imbalance ratio
    ratios = [demographics[split]['Pneumonia']/demographics[split]['Total'] for split in splits]
    axes[1,1].bar(splits, ratios, color=['lightgreen', 'orange', 'lightcoral'])
    axes[1,1].set_title('Pneumonia Ratio by Split')
    axes[1,1].set_ylabel('Pneumonia Ratio')
    axes[1,1].axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Balanced Line')
    axes[1,1].legend()
    
    plt.tight_layout()
    plt.show()
    
    return demographics

def create_data_generators(train_dir, val_dir, test_dir):
    """Create data generators with augmentation for training"""
    
    # Data augmentation for training
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        horizontal_flip=True,
        zoom_range=0.2,
        shear_range=0.2,
        fill_mode='nearest'
    )
    
    # Only rescaling for validation and test
    val_test_datagen = ImageDataGenerator(rescale=1./255)
    
    # Create generators
    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=config.IMG_SIZE,
        batch_size=config.BATCH_SIZE,
        class_mode='binary',
        shuffle=True
    )
    
    val_generator = val_test_datagen.flow_from_directory(
        val_dir,
        target_size=config.IMG_SIZE,
        batch_size=config.BATCH_SIZE,
        class_mode='binary',
        shuffle=False
    )
    
    test_generator = val_test_datagen.flow_from_directory(
        test_dir,
        target_size=config.IMG_SIZE,
        batch_size=config.BATCH_SIZE,
        class_mode='binary',
        shuffle=False
    )
    
    return train_generator, val_generator, test_generator

def build_model():
    """Build the CNN model using transfer learning"""
    
    # Use pre-trained VGG16 as base
    base_model = VGG16(
        weights='imagenet',
        include_top=False,
        input_shape=(*config.IMG_SIZE, 3)
    )
    
    # Freeze base model layers
    base_model.trainable = False
    
    # Add custom classification head
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dropout(0.5),
        layers.Dense(512, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(1, activation='sigmoid')
    ])
    
    return model

def compile_model(model, train_generator):
    """Compile the model with appropriate loss and metrics"""
    
    # Calculate class weights to handle imbalance
    class_weights = compute_class_weight(
        'balanced',
        classes=np.unique(train_generator.classes),
        y=train_generator.classes
    )
    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
    
    model.compile(
        optimizer=Adam(learning_rate=config.LEARNING_RATE),
        loss='binary_crossentropy',
        metrics=['accuracy', 'precision', 'recall']
    )
    
    return model, class_weight_dict

def create_callbacks():
    """Create training callbacks"""
    
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=5,
            min_lr=0.00001,
            verbose=1
        ),
        ModelCheckpoint(
            f'{config.MODEL_NAME}_best.h5',
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        )
    ]
    
    return callbacks

def train_model(model, train_generator, val_generator, class_weight_dict, callbacks):
    """Train the model"""
    
    print("üöÄ STARTING MODEL TRAINING")
    print("=" * 50)
    
    history = model.fit(
        train_generator,
        epochs=config.EPOCHS,
        validation_data=val_generator,
        class_weight=class_weight_dict,
        callbacks=callbacks,
        verbose=1
    )
    
    return history

def plot_training_history(history):
    """Plot training history"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Plot training & validation accuracy
    axes[0,0].plot(history.history['accuracy'], label='Training Accuracy')
    axes[0,0].plot(history.history['val_accuracy'], label='Validation Accuracy')
    axes[0,0].set_title('Model Accuracy')
    axes[0,0].set_xlabel('Epoch')
    axes[0,0].set_ylabel('Accuracy')
    axes[0,0].legend()
    axes[0,0].grid(True)
    
    # Plot training & validation loss
    axes[0,1].plot(history.history['loss'], label='Training Loss')
    axes[0,1].plot(history.history['val_loss'], label='Validation Loss')
    axes[0,1].set_title('Model Loss')
    axes[0,1].set_xlabel('Epoch')
    axes[0,1].set_ylabel('Loss')
    axes[0,1].legend()
    axes[0,1].grid(True)
    
    # Plot precision
    axes[1,0].plot(history.history['precision'], label='Training Precision')
    axes[1,0].plot(history.history['val_precision'], label='Validation Precision')
    axes[1,0].set_title('Model Precision')
    axes[1,0].set_xlabel('Epoch')
    axes[1,0].set_ylabel('Precision')
    axes[1,0].legend()
    axes[1,0].grid(True)
    
    # Plot recall
    axes[1,1].plot(history.history['recall'], label='Training Recall')
    axes[1,1].plot(history.history['val_recall'], label='Validation Recall')
    axes[1,1].set_title('Model Recall')
    axes[1,1].set_xlabel('Epoch')
    axes[1,1].set_ylabel('Recall')
    axes[1,1].legend()
    axes[1,1].grid(True)
    
    plt.tight_layout()
    plt.show()

def evaluate_model(model, test_generator):
    """Evaluate the model on test data"""
    
    print("üìà MODEL EVALUATION ON TEST SET")
    print("=" * 50)
    
    # Get predictions
    test_generator.reset()
    predictions = model.predict(test_generator, verbose=1)
    predicted_classes = (predictions > 0.5).astype(int).flatten()
    true_classes = test_generator.classes
    
    # Calculate metrics
    test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_generator, verbose=0)
    f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)
    
    print(f"Test Accuracy: {test_accuracy:.4f}")
    print(f"Test Precision: {test_precision:.4f}")
    print(f"Test Recall: {test_recall:.4f}")
    print(f"Test F1-Score: {f1_score:.4f}")
    
    return predictions, predicted_classes, true_classes

def plot_evaluation_results(predictions, predicted_classes, true_classes, test_generator):
    """Plot comprehensive evaluation results"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Confusion Matrix
    cm = confusion_matrix(true_classes, predicted_classes)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])
    axes[0,0].set_title('Confusion Matrix')
    axes[0,0].set_xlabel('Predicted')
    axes[0,0].set_ylabel('Actual')
    axes[0,0].set_xticklabels(['Normal', 'Pneumonia'])
    axes[0,0].set_yticklabels(['Normal', 'Pneumonia'])
    
    # ROC Curve
    fpr, tpr, _ = roc_curve(true_classes, predictions)
    roc_auc = auc(fpr, tpr)
    axes[0,1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    axes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    axes[0,1].set_xlim([0.0, 1.0])
    axes[0,1].set_ylim([0.0, 1.05])
    axes[0,1].set_xlabel('False Positive Rate')
    axes[0,1].set_ylabel('True Positive Rate')
    axes[0,1].set_title('ROC Curve')
    axes[0,1].legend(loc="lower right")
    axes[0,1].grid(True)
    
    # Prediction Distribution
    axes[1,0].hist(predictions[true_classes==0], bins=30, alpha=0.7, label='Normal', color='skyblue')
    axes[1,0].hist(predictions[true_classes==1], bins=30, alpha=0.7, label='Pneumonia', color='salmon')
    axes[1,0].axvline(x=0.5, color='red', linestyle='--', label='Threshold')
    axes[1,0].set_xlabel('Predicted Probability')
    axes[1,0].set_ylabel('Frequency')
    axes[1,0].set_title('Prediction Probability Distribution')
    axes[1,0].legend()
    axes[1,0].grid(True)
    
    # Classification Report Heatmap
    from sklearn.metrics import precision_recall_fscore_support
    precision, recall, f1, support = precision_recall_fscore_support(true_classes, predicted_classes)
    
    metrics_data = np.array([[precision[0], recall[0], f1[0]],
                            [precision[1], recall[1], f1[1]]])
    
    sns.heatmap(metrics_data, annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[1,1])
    axes[1,1].set_title('Classification Metrics by Class')
    axes[1,1].set_xlabel('Metrics')
    axes[1,1].set_ylabel('Classes')
    axes[1,1].set_xticklabels(['Precision', 'Recall', 'F1-Score'])
    axes[1,1].set_yticklabels(['Normal', 'Pneumonia'])
    
    plt.tight_layout()
    plt.show()

def display_sample_predictions(model, test_generator, num_samples=8):
    """Display sample predictions with images"""
    
    # Get a batch of test images
    test_generator.reset()
    batch_images, batch_labels = next(test_generator)
    
    # Make predictions
    predictions = model.predict(batch_images)
    
    # Create subplots
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    axes = axes.ravel()
    
    for i in range(min(num_samples, len(batch_images))):
        # Display image
        axes[i].imshow(batch_images[i])
        
        # Get prediction
        pred_prob = predictions[i][0]
        pred_class = 'Pneumonia' if pred_prob > 0.5 else 'Normal'
        true_class = 'Pneumonia' if batch_labels[i] == 1 else 'Normal'
        
        # Set title with prediction info
        color = 'green' if pred_class == true_class else 'red'
        axes[i].set_title(f'True: {true_class}\nPred: {pred_class} ({pred_prob:.3f})', 
                         color=color, fontsize=10)
        axes[i].axis('off')
    
    plt.tight_layout()
    plt.show()

def save_results(history, demographics):
    """Save training results and demographics to files"""
    
    # Save training history
    history_df = pd.DataFrame(history.history)
    history_df.to_csv('training_history.csv', index=False)
    
    # Save demographics
    demo_df = pd.DataFrame(demographics).T
    demo_df.to_csv('dataset_demographics.csv')
    
    print("üìÅ Results saved to:")
    print("  - training_history.csv")
    print("  - dataset_demographics.csv")
    print(f"  - {config.MODEL_NAME}_best.h5")

def main():
    """Main execution function"""
    
    print("üî¨ PNEUMONIA DETECTION SYSTEM")
    print("=" * 50)
    
    # Setup paths
    train_dir, val_dir, test_dir = setup_data_paths()
    
    # Analyze demographics
    demographics = analyze_dataset_demographics(train_dir, val_dir, test_dir)
    
    # Create data generators
    train_gen, val_gen, test_gen = create_data_generators(train_dir, val_dir, test_dir)
    
    # Build and compile model
    model = build_model()
    model, class_weights = compile_model(model, train_gen)
    
    # Print model summary
    print("\nüìã MODEL ARCHITECTURE:")
    model.summary()
    
    # Create callbacks
    callbacks = create_callbacks()
    
    # Train model
    history = train_model(model, train_gen, val_gen, class_weights, callbacks)
    
    # Plot training history
    plot_training_history(history)
    
    # Evaluate model
    predictions, pred_classes, true_classes = evaluate_model(model, test_gen)
    
    # Plot evaluation results
    plot_evaluation_results(predictions, pred_classes, true_classes, test_gen)
    
    # Display sample predictions
    display_sample_predictions(model, test_gen)
    
    # Save results
    save_results(history, demographics)
    
    print("\n‚úÖ TRAINING COMPLETE!")
    print("Check the generated plots and saved files for detailed results.")

# Run the complete pipeline
if __name__ == "__main__":
    main()
